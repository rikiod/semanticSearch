{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files:\n",
      "150B Fall 2023 - Syllabus (1TfRgTBKC4tSOZ0p5HGt5W7xYzXmSJQBFRxAZsR9xe1k)\n",
      "Assessment Registry (1ZwrE4AV0rsp6dk_2q4b2dmAWnuMzcx3L4CHVKm6jTUo)\n",
      "Drop-in Hours (BPH Academic Advisor) (1Ml6lpkZ3TrqAj6r8Jza5gAX8qqyYz9HTMvPcpGQppgE)\n",
      "SMUHSD Scholarship Opportunities 2023-24 (14GnmebMh-TpoIxSkKYq5gpbVrhbIIVrGRIi0BzgLMF0)\n",
      "Resources for Student - Public Health Interest (1a0zHJH3SvJwCvPgYiwff114FXcx-N7tmCcPuNfk3uDs)\n",
      "ISAB Committee Descriptions (1Sg1axAdJqOkUAAXKsxO8uCB4ZNxm8FhfTraajbo2vt8)\n",
      "Miau_GameVideo.mp4 (19coyvk_TOaqfhVSo8Sj9V_4dYN63VFPh)\n",
      "Month Dinners (1KRvR9J_uH-Vozq5mirfaLgB79M92vPSfH8Mb0Hw4P8M)\n",
      "Comprehensive Recruitment Preparation FAQ (1ahxtZDqLib7-kKSa5zcsztxx4TaAsszIcWR_xwCFJsI)\n",
      "Public Resume for Review- Latest (1lS2m7xpqoZHTOxIQb2Y0VYACIs8L4ed4tRF8weO-Uvw)\n"
     ]
    }
   ],
   "source": [
    "# connecting to Google Drive API\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly']\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Shows basic usage of the Drive v3 API.\n",
    "    Prints the names and ids of the first 10 files the user has access to.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    try:\n",
    "        service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "        # Call the Drive v3 API\n",
    "        results = service.files().list(\n",
    "            pageSize=10, fields=\"nextPageToken, files(id, name)\").execute()\n",
    "        items = results.get('files', [])\n",
    "\n",
    "        if not items:\n",
    "            print('No files found.')\n",
    "            return\n",
    "        print('Files:')\n",
    "        for item in items:\n",
    "            print(u'{0} ({1})'.format(item['name'], item['id']))\n",
    "    except HttpError as error:\n",
    "        # TODO(developer) - Handle errors from drive API.\n",
    "        print(f'An error occurred: {error}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note to self --\n",
    "API keys are stored in conda virtual environemnt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 11/11 [00:00<00:00, 23.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# read in pdfs\n",
    "import PyPDF2\n",
    "\n",
    "# takes in a string (example.pdf) and returns a python list with the contents of the pdf \n",
    "def pdf_to_list(pdf_name): # i might need to change this when adding google drive integration\n",
    "    all_text = ''\n",
    "    file = open(pdf_name, 'rb')\n",
    "    reader = PyPDF2.PdfFileReader(file)\n",
    "    \n",
    "    for page_number in range(len(reader.pages)):\n",
    "        page = reader.pages[page_number]\n",
    "        text = page.extract_text()\n",
    "        all_text += text\n",
    "\n",
    "    file.close()\n",
    "    return [all_text]\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm # to track ingestion of pdfs via progress bar\n",
    "import os\n",
    "\n",
    "def ingest_pdfs(folder_path):\n",
    "    data = []\n",
    "\n",
    "    all_files = os.listdir(folder_path) \n",
    "    pdf_files = [file for file in all_files if file.endswith('.pdf')]\n",
    "\n",
    "    for file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        filepath = os.path.join(folder_path, file)\n",
    "        text = pdf_to_list(filepath)\n",
    "        data.append({\"filename\": file, \"directory\": folder_path, \"text\": text})\n",
    "    \n",
    "    df = pd.DataFrame(data=data)\n",
    "    return df \n",
    "\n",
    "folder_path = 'sample_files'\n",
    "df = ingest_pdfs(folder_path)\n",
    "\n",
    "import cohere\n",
    "import os\n",
    "cohere_api_key = os.environ.get('COHERE_API_KEY')\n",
    "co = cohere.Client(cohere_api_key)\n",
    "\n",
    "def embed(text):\n",
    "    embedding = co.embed(texts=text) # using default model, returns embedding object. should i have made a file object instead? would have text, name, embedding \n",
    "    return embedding.embeddings  \n",
    "\n",
    "embeddings = [] \n",
    "for text in df['text'].tolist():\n",
    "    embedding = embed(text)\n",
    "    embeddings += embedding\n",
    "\n",
    "df['embedding'] = embeddings # this relies upon the fact that the list retains its order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should do this if EXPLAIN WHEN YOU NEED TO DO\n",
    "# pinecone.create_index(\"semantic-search\", dimension=4096, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. rikio_resume.pdf\n",
      "2. Green Gulch app.pdf\n",
      "3. Launchpad application.pdf\n"
     ]
    }
   ],
   "source": [
    "import pinecone \n",
    "pinecone_api_key = os.environ.get('PINECONE_API_KEY')\n",
    "\n",
    "pinecone_environment = 'gcp-starter'\n",
    "pinecone.init(api_key=pinecone_api_key, environment=pinecone_environment)\n",
    "\n",
    "# pinecone.create_index(\"semantic-search\", dimension=8, metric=\"euclidean\")\n",
    "index = pinecone.Index(\"semantic-search\")\n",
    "\n",
    "embeddings = df['embedding'].tolist()\n",
    "\n",
    "filenames = df['filename'].tolist()\n",
    "to_upsert = zip(filenames, embeddings)\n",
    "index.upsert(to_upsert) # might need to batch upsert if >100 pdfs \n",
    "\n",
    "user_input = input(\"Please enter your search: \")\n",
    "user_embedding = embed([user_input]) \n",
    "most_similar = 3\n",
    "similar_matches = index.query(user_embedding, top_k=most_similar, include_values=True)\n",
    "similar_matches_filenames = [match['id'] for match in similar_matches['matches']]\n",
    "for i in range(len(similar_matches_filenames)):\n",
    "    print(str(i + 1) + '.' + ' ' + similar_matches_filenames[i])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semanticSearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
